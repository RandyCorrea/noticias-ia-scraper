# üì∞ Noticias IA ‚Äì Sistema de Scraping Autom√°tico con ML Supervisado

**Sistema completo de extracci√≥n de noticias, calificaci√≥n autom√°tica y aprendizaje de m√°quina supervisado.**

Bot autom√°tico cada 6 horas + WebApp Flask privada para calificar noticias + Modelo ML que aprende seg√∫n tus puntuaciones.

---

## ‚ú® Caracter√≠sticas

‚úÖ **Bot Aut√≥m√°tico (GitHub Actions)** ‚Äì Se ejecuta cada 6 horas extrayendo noticias autom√°ticamente  
‚úÖ **WebApp Privada (Flask)** ‚Äì Panel simple con acceso por clave √∫nica, sin usuarios  
‚úÖ **Calificaci√≥n de Noticias** ‚Äì Punt√∫a del 1 al 10, el sistema aprende autom√°ticamente  
‚úÖ **Agregar Portales Din√°micamente** ‚Äì Pega la URL, testea que sea extra√≠ble, y guarda  
‚úÖ **Machine Learning Supervisado** ‚Äì TF-IDF + Ridge, modelo se reentren automaticamente  
‚úÖ **Base de Datos Centralizada** ‚Äì Bot y WebApp se comunican SOLO v√≠a BD  
‚úÖ **Sin Dependencias Externas** ‚Äì SQLite por defecto, puedes cambiar a PostgreSQL  
‚úÖ **Producci√≥n Ready** ‚Äì Gunicorn, variables de entorno, logging  

---

## üèóÔ∏è Estructura de Carpetas

```
noticias-ia-scraper/
‚îú‚îÄ‚îÄ bot/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ scraper.py           # L√≥gica de extracci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ ml.py                # Entrenamiento del modelo
‚îÇ   ‚îú‚îÄ‚îÄ run.py               # Punto de entrada del bot
‚îÇ   ‚îî‚îÄ‚îÄ utils.py             # Utilidades
‚îÇ
‚îú‚îÄ‚îÄ webapp/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ app.py               # Creaci√≥n de app Flask
‚îÇ   ‚îú‚îÄ‚îÄ auth.py              # Autenticaci√≥n por clave
‚îÇ   ‚îú‚îÄ‚îÄ routes.py            # Rutas de la aplicaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ forms.py             # Formularios WTF
‚îÇ   ‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.html
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ login.html
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ home.html        # Panel de noticias
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ portales.html    # Agregar/gestionar portales
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ noticias.html
‚îÇ   ‚îî‚îÄ‚îÄ static/
‚îÇ       ‚îî‚îÄ‚îÄ style.css
‚îÇ
‚îú‚îÄ‚îÄ db/
‚îÇ   ‚îú‚îÄ‚îÄ models.py            # Modelos SQLAlchemy
‚îÇ   ‚îî‚îÄ‚îÄ db_init.py           # Inicializaci√≥n de BD
‚îÇ
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ config.py            # Configuraci√≥n global
‚îÇ
‚îú‚îÄ‚îÄ .github/workflows/
‚îÇ   ‚îî‚îÄ‚îÄ bot.yml              # GitHub Actions (cada 6h)
‚îÇ
‚îú‚îÄ‚îÄ main.py                  # Punto de entrada de la WebApp
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .env.example
‚îî‚îÄ‚îÄ README.md (este archivo)
```

---

## üöÄ Instalaci√≥n R√°pida (5 minutos)

### 1. Clonar el repositorio

```bash
git clone https://github.com/TU_USUARIO/noticias-ia-scraper.git
cd noticias-ia-scraper
```

### 2. Crear variables de entorno

```bash
cp .env.example .env
# Edita .env con tus valores:
# SECRET_KEY = clave aleatoria segura
# ADMIN_PASSWORD = contrase√±a para acceder a la web
# DB_URL = sqlite:///noticias.db (o tu BD de producci√≥n)
```

### 3. Crear y activar entorno virtual

```bash
python3 -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

### 4. Instalar dependencias

```bash
pip install -r requirements.txt
```

### 5. Inicializar base de datos

```bash
python db/db_init.py
```

### 6. Ejecutar la WebApp

```bash
python main.py
# Accede a http://localhost:5000
```

### 7. Ejecutar el Bot manualmente (opcional)

```bash
python bot/run.py
```

---

## üéÆ Uso de la WebApp

### üìß Login
- URL: `http://localhost:5000`
- Introduce tu `ADMIN_PASSWORD` del archivo `.env`
- Sin usuarios, sin registros, sin correos

### üì∞ Panel de Noticias
- Se cargan noticias autom√°ticamente desde los portales activos
- **Califica del 1 al 10** haciendo clic en los n√∫meros
- El sistema aprende seg√∫n tus puntuaciones

### üîó Agregar Portales
1. Ve a la secci√≥n "Portales"
2. **Pega la URL** del portal (ej: https://www.eluniversal.com.ve)
3. Selecciona la **categor√≠a**
4. Haz clic en **"TESTEAR EXTRACCI√ìN"**
   - Si es exitoso ‚Üí se muestra cantidad de t√≠tulos y ejemplos
   - Si falla ‚Üí ve el motivo (anti-bot, JS din√°mico, etc.)
5. Si el test es OK, haz clic en **"GUARDAR"**
6. El portal se agregar√° y el bot lo incluir√° en la pr√≥xima ejecuci√≥n

### üóëÔ∏è Eliminar Portales
- En la lista de portales, haz clic en el bot√≥n "Eliminar"

---

## ü§ñ Flujo del Bot Autom√°tico

Cada 6 horas (configurable en `.github/workflows/bot.yml`):

1. **Lee portales activos** desde la BD
2. **Extrae titulares** de cada portal
3. **Evita duplicados** checando t√≠tulos y enlaces
4. **Usa ML** para asignar score predictivo de relevancia (1-10)
5. **Guarda** noticias en la BD
6. **Reentren el modelo** si hay nuevas calificaciones tuyas

---

## üß† Machine Learning

- **Algoritmo**: TF-IDF (vectorizaci√≥n) + Ridge (regresi√≥n)
- **Entrada**: T√≠tulo + Resumen de noticia
- **Salida**: Score predicho de relevancia (1-10)
- **Entrenamiento**: Supervisado con tus calificaciones manuales
- **Reentrenamiento**: Autom√°tico cada ciclo del bot (si hay nuevos votos)
- **Persistencia**: Modelo guardado en `bot/modelo.pkl`

---

## ‚öôÔ∏è Configuraci√≥n en Producci√≥n

### 1. Cambiar a PostgreSQL

```bash
# Instala el driver
pip install psycopg2-binary

# En .env:
DB_URL=postgresql://user:password@localhost:5432/noticias_ia
```

### 2. Deploy con Gunicorn

```bash
gunicorn main:app --workers 4 --bind 0.0.0.0:8000
```

### 3. Configurar GitHub Secrets

En `Settings > Secrets and variables > Actions`, agrega:

```
DB_URL = tu_url_bd_produccion
ADMIN_PASSWORD = tu_contrasena
SECRET_KEY = clave_aleatoria
```

### 4. Workflow autom√°tico en GitHub

El archivo `.github/workflows/bot.yml` ya est√° configurado para ejecutarse cada 6 horas.

---

## üìä Base de Datos

### Tablas principales

**fuentes** (portales de noticias)
```
id (PK) | url | categoria | activa
```

**noticias** (titulares extra√≠dos)
```
id (PK) | titulo | resumen | link | fecha | fuente (FK) | score_predicho | entrenada
```

**votos** (calificaciones manuales)
```
id (PK) | noticia_id (FK) | calificacion_1_a_10
```

**modelo_estado** (metadata del entrenamient)
```
id (PK) | fecha_entrenamiento
```

---

## üîí Seguridad

- ‚úÖ Acceso por contrase√±a √∫nica (sin usuarios)
- ‚úÖ Sesiones Flask seguras
- ‚úÖ Variables de entorno (no hardcodeadas)
- ‚úÖ CSRF protection en formularios
- ‚úÖ SQL injection prevenci√≥n (ORM SQLAlchemy)
- ‚ö†Ô∏è **En producci√≥n**: Usa HTTPS, cambiar `SECRET_KEY`, usar BD segura

---

## üêõ Troubleshooting

### "No puedo acceder a la web"
- Verifica que `ADMIN_PASSWORD` en `.env` es correcto
- Checa que el puerto 5000 est√© disponible

### "El bot no extrae noticias"
- Verifica que los portales est√°n activos en la BD
- Revisa logs de la GitHub Actions
- Algunas webs tienen protecci√≥n anti-bot

### "El modelo no mejora"
- Necesitas al menos 10-15 noticias calificadas para entrenar
- Verifica que est√°s calificando noticias (debe marcar "entrenada")

### "Error de conexi√≥n a BD"
- Checa que el archivo `noticias.db` exista
- Si usas PostgreSQL, verifica credenciales en `DB_URL`

---

## üìù Ejemplo de uso completo

```bash
# 1. Clonar y setup
git clone TU_REPO && cd noticias-ia-scraper
cp .env.example .env
# Edita .env
pip install -r requirements.txt
python db/db_init.py

# 2. Ejecutar web
python main.py
# Accede a http://localhost:5000 con tu contrase√±a

# 3. Agregar portales
# - Ve a /portales
# - Pega: https://eluniversal.com.ve
# - Testea
# - Guarda

# 4. Ejecutar bot manualmente
python bot/run.py
# Las noticias aparecen en /home

# 5. Calificar noticias
# - Haz clic en n√∫meros 1-10 para cada noticia
# - El modelo aprende autom√°ticamente

# 6. GitHub Actions autom√°tico
# - Bot corre cada 6h sin que hagas nada
```

---

## üõ†Ô∏è API de Rutas (WebApp)

| Ruta | M√©todo | Descripci√≥n |
|------|--------|-------------|
| `/` | GET/POST | Login |
| `/home` | GET | Panel de noticias para calificar |
| `/portales` | GET/POST | Gestionar portales |
| `/noticia/<id>/califica/<score>` | GET | Calificar noticia (1-10) |
| `/test_portal` | POST | Testear extracci√≥n de portal |
| `/agrega_portal` | POST | Guardar nuevo portal |
| `/elimina_portal/<id>` | POST | Eliminar portal |
| `/logout` | GET | Cerrar sesi√≥n |

---

## üì¶ Dependencias Principales

- **Flask** ‚Äì Framework web
- **SQLAlchemy** ‚Äì ORM para BD
- **Scikit-Learn** ‚Äì ML (TF-IDF + Ridge)
- **Newspaper3k** ‚Äì Extracci√≥n de noticias
- **APScheduler** ‚Äì Scheduling de tareas
- **Gunicorn** ‚Äì Servidor WSGI producci√≥n

---

## üåê Portales Recomendados para Pruebas

```
Pol√≠tica: https://www.eluniversal.com.ve
Tecnolog√≠a: https://www.techcrunch.com
Deportes: https://www.espn.com
Econom√≠a: https://www.bloomberg.com
Crimen: https://www.bbc.com
```

---

## üìÑ Licencia

MIT License ‚Äì √ösalo libremente

---

## ü§ù Contribuir

Encontraste un bug o tienes una idea? Abre un [Issue](https://github.com/TU_USUARIO/noticias-ia-scraper/issues) o un [Pull Request](https://github.com/TU_USUARIO/noticias-ia-scraper/pulls).

---

## üìû Soporte

- Abre un Issue en GitHub
- Revisa la secci√≥n Troubleshooting
- Verifica los logs de GitHub Actions

---

**Hecho con ‚ù§Ô∏è para scrapers y ML enthusiasts**
